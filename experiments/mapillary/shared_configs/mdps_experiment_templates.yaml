experiment_templates:


  - MDPS_Training_Filters_Base_Template:

      # The parameters for the device to use
      # This specifies which GPU or how many GPUs we want and also what the requirements are for the GPU
      device_configs:
        # device: "cuda_auto_multi:1"
        device: "cuda_use_all"
        min_free_memory_gb: 1.0
        max_number_of_tasks: 10

      # Specify the configs needed for training
      training_configs:       

        # The configs that specify early stopping
        early_stopping_configs:
          patience: 10
          start_offset: 25
          min_delta: 0
          max_lr_change: 3


        # The configs for the loss.  At minimum you need to specify the "name". 
        # All additional configs are metric specific
        loss_configs: 
          name: CombinedMetric

          metric_configs:
            - metric_forward:
                name: KDENLLMetric
                alpha: 0.5
                output_keys:
                  particles: "forward_particles"
                  particle_weights: "forward_particle_weights"
                  bandwidths: "forward_bandwidths"

                particle_extract_dim_params:
                  0: 
                    dim_in_kde: 0
                    kde_distribution_type: "Normal"
                  1: 
                    dim_in_kde: 1
                    kde_distribution_type: "Normal"
                  2: 
                    dim_in_kde: 2
                    kde_distribution_type: "VonMises"

            - metric_backward:
                name: KDENLLMetric
                alpha: 0.5
                output_keys:
                  particles: "backward_particles"
                  particle_weights: "backward_particle_weights"
                  bandwidths: "backward_bandwidths"

                particle_extract_dim_params:
                  0: 
                    dim_in_kde: 0
                    kde_distribution_type: "Normal"
                  1: 
                    dim_in_kde: 1
                    kde_distribution_type: "Normal"
                  2: 
                    dim_in_kde: 2
                    kde_distribution_type: "VonMises"




  - MDPS_Training_Filters_Stage_1_Template:
    
      # The config file for the dataset we want to use 
      dataset_configs_file: "../experiments/mapillary/shared_configs/dataset_mapillary_custom_split_sequences_short.yaml"


      # The save dir
      save_dir: <root_save_dir>/001_training_filters_stage_1/

      # Specify the configs needed for training
      training_configs:       

        # The batch sizes for training per GPU
        # This will automatically scale the batch size when we scale the number of GPUs
        batch_sizes_per_gpu:
          training: 3
          validation: 3

        # These are the learning rates for the internal models
        # If you want to use 1 learnung rate for all the models you can set "full_model" but then there can be no other entries
        learning_rates:
          forward_output_bandwidth_model:       "freeze"
          backward_output_bandwidth_model:      "freeze"
          output_bandwidth_model:               "freeze"
          output_weights_model:                 "freeze"

          forward_resampling_bandwidth_model:   0.00001
          backward_resampling_bandwidth_model:  0.00001
          
          forward_dynamics_model:               0.001
          backward_dynamics_model:              0.001
          forward_output_weights_model:         0.001
          backward_output_weights_model:        0.001
          forward_resampling_weights_model:     0.001
          backward_resampling_weights_model:    0.001
          
          map_encoder_model:                    0.001
          observation_encoder_model:            0.001




  - MDPS_Training_Filters_Stage_2_Template:
    
      # The save dir
      save_dir: <root_save_dir>/002_training_filters_stage_2/

      # The config file for the dataset we want to use 
      dataset_configs_file: "../experiments/mapillary/shared_configs/dataset_mapillary_custom_split_sequences_long.yaml"
      # dataset_configs_file: "../experiments/mapillary/shared_configs/dataset_mapillary_custom_split_sequences_short.yaml"

      # Specify which model to load
      pretrained_models:
        full_model: <root_save_dir>/001_training_filters_stage_1/<framework_var_run_number>/models/best/full_model.pt

      # Specify the configs needed for training
      training_configs:       

        # The batch sizes for training per GPU
        # This will automatically scale the batch size when we scale the number of GPUs
        batch_sizes_per_gpu:
          training: 2
          validation: 2

        # How many epochs to train for
        epochs: 5

        # These are the learning rates for the internal models
        # If you want to use 1 learnung rate for all the models you can set "full_model" but then there can be no other entries
        learning_rates:
          forward_output_bandwidth_model:       0.01
          backward_output_bandwidth_model:      0.01
          forward_resampling_bandwidth_model:   "freeze"
          backward_resampling_bandwidth_model:  "freeze"
          output_bandwidth_model:               "freeze"
          forward_dynamics_model:               "freeze"
          backward_dynamics_model:              "freeze"
          forward_output_weights_model:         "freeze"
          backward_output_weights_model:        "freeze"
          forward_resampling_weights_model:     "freeze"
          backward_resampling_weights_model:    "freeze"
          output_weights_model:                 "freeze"
          map_encoder_model:                    "freeze"
          observation_encoder_model:            "freeze"

        models_with_disabled_gradients:
          - observation_encoder_model
          - map_encoder_model
          - forward_resampling_bandwidth_model
          - backward_resampling_bandwidth_model
          - output_bandwidth_model
          - forward_dynamics_model
          - backward_dynamics_model
          - forward_output_weights_model
          - backward_output_weights_model
          - forward_resampling_weights_model
          - backward_resampling_weights_model
          - output_weights_model



  - MDPS_Training_Weights_Template:

      # The config file for the dataset we want to use 
      dataset_configs_file: "../experiments/mapillary/shared_configs/dataset_mapillary_custom_split_sequences_long.yaml"
      # dataset_configs_file: "../experiments/mapillary/shared_configs/dataset_mapillary_custom_split_sequences_short.yaml"

      # The save dir
      save_dir: <root_save_dir>/003_training_weights/


      # Specify which model to load
      pretrained_models:
        full_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/full_model.pt
        # forward_output_bandwidth_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/forward_output_bandwidth_model.pt
        # backward_output_bandwidth_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/backward_output_bandwidth_model.pt
        # forward_resampling_bandwidth_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/forward_resampling_bandwidth_model.pt
        # backward_resampling_bandwidth_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/backward_resampling_bandwidth_model.pt
        # forward_dynamics_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/forward_dynamics_model.pt
        # backward_dynamics_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/backward_dynamics_model.pt
        # forward_output_weights_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/forward_output_weights_model.pt
        # backward_output_weights_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/backward_output_weights_model.pt
        # forward_resampling_weights_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/forward_resampling_weights_model.pt
        # backward_resampling_weights_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/backward_resampling_weights_model.pt
        # observation_encoder_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/observation_encoder_model.pt
        # map_encoder_model: <root_save_dir>/002_training_filters_stage_2/<framework_var_run_number>/models/best/map_encoder_model.pt

      # The parameters for the device to use
      # This specifies which GPU or how many GPUs we want and also what the requirements are for the GPU
      device_configs:
        # device: "cuda_auto_multi:1"
        device: "cuda_use_all"
        min_free_memory_gb: 1.0
        max_number_of_tasks: 10

      # Specify the configs needed for training
      training_configs:       

        # The batch sizes for training per GPU
        # This will automatically scale the batch size when we scale the number of GPUs
        batch_sizes_per_gpu:
          training: 2
          validation: 2

        num_cpu_cores_for_dataloader: 8

        # These are the learning rates for the internal models
        # If you want to use 1 learnung rate for all the models you can set "full_model" but then there can be no other entries
        learning_rates:
          forward_output_bandwidth_model:       "freeze"
          backward_output_bandwidth_model:      "freeze"
          forward_resampling_bandwidth_model:   "freeze"
          backward_resampling_bandwidth_model:  "freeze"
          forward_dynamics_model:               "freeze"
          backward_dynamics_model:              "freeze"
          forward_output_weights_model:         "freeze"
          backward_output_weights_model:        "freeze"
          forward_resampling_weights_model:     "freeze"
          backward_resampling_weights_model:    "freeze"
          observation_encoder_model:            "freeze"
          map_encoder_model:                    "freeze"

          output_weights_model:                 0.001
          output_bandwidth_model:               0.0001

        models_with_disabled_gradients:
          - observation_encoder_model
          - map_encoder_model
          - forward_output_bandwidth_model
          - backward_output_bandwidth_model
          - forward_resampling_bandwidth_model
          - backward_resampling_bandwidth_model
          - forward_dynamics_model
          - backward_dynamics_model
          - forward_output_weights_model
          - backward_output_weights_model
          - forward_resampling_weights_model
          - backward_resampling_weights_model

        # The configs for the loss.  At minimum you need to specify the "name". 
        # All additional configs are metric specific
        loss_configs: 
          name: KDENLLMetric
          output_keys:
            particles: "particles"
            particle_weights: "particle_weights"
            bandwidths: "bandwidths"

          particle_extract_dim_params:
            0: 
              dim_in_kde: 0
              kde_distribution_type: "Normal"
            1: 
              dim_in_kde: 1
              kde_distribution_type: "Normal"
            2: 
              dim_in_kde: 2
              kde_distribution_type: "VonMises"





  # - MDPS_Training_all_Template:

  #     # The config file for the dataset we want to use 
  #     dataset_configs_file: "../experiments/mapillary/shared_configs/dataset_mapillary_custom_split_sequences_long.yaml"

  #     # The save dir
  #     save_dir: <root_save_dir>/004_training_all/


  #     # Specify which model to load
  #     # pretrained_models:
  #       # full_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/full_model.pt


  #     # The parameters for the device to use
  #     # This specifies which GPU or how many GPUs we want and also what the requirements are for the GPU
  #     device_configs:
  #       # device: "cuda_auto_multi:1"
  #       device: "cuda_use_all"
  #       min_free_memory_gb: 1.0
  #       max_number_of_tasks: 10


  #     pretrained_models:
  #       # full_model: <root_save_dir>/001_training_filters_stage_1/<framework_var_run_number>/models/best/full_model.pt

  #       forward_output_bandwidth_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/forward_output_bandwidth_model.pt
  #       backward_output_bandwidth_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/backward_output_bandwidth_model.pt
  #       observation_encoder_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/observation_encoder_model.pt
  #       map_encoder_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/map_encoder_model.pt

  #       forward_resampling_bandwidth_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/forward_resampling_bandwidth_model.pt
  #       backward_resampling_bandwidth_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/backward_resampling_bandwidth_model.pt
  #       forward_dynamics_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/forward_dynamics_model.pt
  #       backward_dynamics_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/backward_dynamics_model.pt
  #       forward_output_weights_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/forward_output_weights_model.pt
  #       backward_output_weights_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/backward_output_weights_model.pt
  #       forward_resampling_weights_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/forward_resampling_weights_model.pt
  #       backward_resampling_weights_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/backward_resampling_weights_model.pt        
  #       output_weights_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/output_weights_model.pt        



  #     # Specify the configs needed for training
  #     training_configs:       

  #       # data_plotter_plot_modulo: 50

  #       # The batch sizes for training per GPU
  #       # This will automatically scale the batch size when we scale the number of GPUs
  #       batch_sizes_per_gpu:
  #         training: 2
  #         validation: 2

  #       num_cpu_cores_for_dataloader: 12

  #       # These are the learning rates for the internal models
  #       # If you want to use 1 learnung rate for all the models you can set "full_model" but then there can be no other entries
  #       learning_rates:
  #         forward_output_bandwidth_model:       "freeze"
  #         backward_output_bandwidth_model:      "freeze"
  #         observation_encoder_model:            "freeze"
  #         map_encoder_model:                    "freeze"


  #         forward_resampling_bandwidth_model:   0.00001
  #         backward_resampling_bandwidth_model:  0.00001
  #         forward_dynamics_model:               0.001
  #         backward_dynamics_model:              0.001
  #         forward_output_weights_model:         0.001
  #         backward_output_weights_model:        0.001
  #         forward_resampling_weights_model:     0.001
  #         backward_resampling_weights_model:    0.001

  #         output_weights_model:                 0.001
  #         output_bandwidth_model:               0.001

  #       models_with_disabled_gradients:
  #         - observation_encoder_model
  #         - map_encoder_model
  #         - forward_output_bandwidth_model
  #         - backward_output_bandwidth_model

  #       # The configs for the loss.  At minimum you need to specify the "name". 
  #       # All additional configs are metric specific
  #       loss_configs: 
  #         name: KDENLLMetric
  #         output_keys:
  #           particles: "particles"
  #           particle_weights: "particle_weights"
  #           bandwidths: "bandwidths"

  #         particle_extract_dim_params:
  #           0: 
  #             dim_in_kde: 0
  #             kde_distribution_type: "Normal"
  #           1: 
  #             dim_in_kde: 1
  #             kde_distribution_type: "Normal"
  #           2: 
  #             dim_in_kde: 2
  #             kde_distribution_type: "VonMises"




  - MDPS_Training_all_Template:

      # The config file for the dataset we want to use 
      dataset_configs_file: "../experiments/mapillary/shared_configs/dataset_mapillary_custom_split_sequences_long.yaml"

      # The save dir
      save_dir: <root_save_dir>/004_training_all/

      # The parameters for the device to use
      # This specifies which GPU or how many GPUs we want and also what the requirements are for the GPU
      device_configs:
        # device: "cuda_auto_multi:1"
        device: "cuda_use_all"
        min_free_memory_gb: 1.0
        max_number_of_tasks: 10


      pretrained_models:
        full_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/full_model.pt

        # forward_output_bandwidth_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/forward_output_bandwidth_model.pt
        # backward_output_bandwidth_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/backward_output_bandwidth_model.pt
        # observation_encoder_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/observation_encoder_model.pt
        # map_encoder_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/map_encoder_model.pt

        # forward_resampling_bandwidth_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/forward_resampling_bandwidth_model.pt
        # backward_resampling_bandwidth_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/backward_resampling_bandwidth_model.pt
        # forward_dynamics_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/forward_dynamics_model.pt
        # backward_dynamics_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/backward_dynamics_model.pt
        # forward_output_weights_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/forward_output_weights_model.pt
        # backward_output_weights_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/backward_output_weights_model.pt
        # forward_resampling_weights_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/forward_resampling_weights_model.pt
        # backward_resampling_weights_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/backward_resampling_weights_model.pt        
        # output_weights_model: <root_save_dir>/003_training_weights/<framework_var_run_number>/models/best/output_weights_model.pt        



      # Specify the configs needed for training
      training_configs:       

        # data_plotter_plot_modulo: 50

        # The batch sizes for training per GPU
        # This will automatically scale the batch size when we scale the number of GPUs
        batch_sizes_per_gpu:
          training: 2
          validation: 2

        num_cpu_cores_for_dataloader: 12

        # These are the learning rates for the internal models
        # If you want to use 1 learnung rate for all the models you can set "full_model" but then there can be no other entries
        learning_rates:
          observation_encoder_model:            "freeze"
          map_encoder_model:                    "freeze"

          forward_resampling_bandwidth_model:   0.0001
          backward_resampling_bandwidth_model:  0.0001
          forward_output_bandwidth_model:       0.0001
          backward_output_bandwidth_model:      0.0001
          forward_dynamics_model:               0.001
          backward_dynamics_model:              0.001
          forward_output_weights_model:         0.001
          backward_output_weights_model:        0.001
          forward_resampling_weights_model:     0.001
          backward_resampling_weights_model:    0.001

          output_weights_model:                 0.001
          output_bandwidth_model:               0.001

        models_with_disabled_gradients:
          - observation_encoder_model
          - map_encoder_model

        # The configs for the loss.  At minimum you need to specify the "name". 
        # All additional configs are metric specific
        loss_configs: 
          name: KDENLLMetric
          output_keys:
            particles: "particles"
            particle_weights: "particle_weights"
            bandwidths: "bandwidths"

          particle_extract_dim_params:
            0: 
              dim_in_kde: 0
              kde_distribution_type: "Normal"
            1: 
              dim_in_kde: 1
              kde_distribution_type: "Normal"
            2: 
              dim_in_kde: 2
              kde_distribution_type: "VonMises"








  - MDPS_Evaluation_Template:

      # The parameters for the device to use
      # This specifies which GPU or how many GPUs we want and also what the requirements are for the GPU
      device_configs:
        # device: "cuda_auto_multi:1"
        device: "cuda_use_all"
        min_free_memory_gb: 1.0
        max_number_of_tasks: 10

      evaluation_configs:         


        # num_cpu_cores_for_dataloader: 16
        num_cpu_cores_for_dataloader: 4

        # The configs for the qualitative phase
        qualitative_config:

          # If we should run the qualitative
          do_run: True

          save_model_input_output_for_rendering_config:
            do_save: True
            number_to_save: 50

            # These are all the sequences that are forward facing
            sequences_to_save: [1, 5, 6, 7, 11, 12, 21, 26, 27, 28, 46, 47, 49, 50, 52, 53, 54, 59, 60, 62, 63, 64, 69, 74, 79, 85, 88, 90, 91, 96, 97, 106, 107, 114, 120, 121, 124, 125, 126, 127, 130, 131, 132, 133, 136, 137, 140, 149, 150, 158, 159, 163, 164, 165, 166, 167, 172, 173, 176, 178, 179, 180, 181, 182, 189, 191, 193, 194, 195, 196, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 212, 220, 222, 225, 232, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 263, 265, 266, 267, 268, 272, 273, 274, 275, 276, 278, 279, 280, 281, 283, 284, 287, 288, 289, 290, 291, 292, 293, 294, 296, 304, 311, 312, 314, 315, 320, 322, 323, 324, 325, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 343, 348, 349, 350, 351, 353, 354, 357, 363, 364, 365, 370, 373, 377, 378, 379, 380, 382, 383, 384, 385, 386, 388, 389, 401, 409, 410, 416, 417, 419, 420, 421, 427, 428, 430, 444, 445, 456, 463, 464, 469, 470, 471, 472, 475, 479, 483, 484, 488, 492, 493, 498, 499, 500, 501, 502, 507, 508, 509, 510, 512, 518, 522, 525, 526, 528, 534, 538, 540, 541, 542, 547, 550, 553, 554, 558, 564, 565, 572, 575, 580, 587, 594, 595, 596, 597, 599, 600, 601, 602, 609, 613, 614, 618, 619, 625, 626, 634, 635, 640, 641, 646, 647, 648, 649, 651, 655, 656, 657, 661, 662, 663, 664, 665, 666, 667, 668, 669, 674, 680, 681, 682, 685, 686, 691, 692, 695, 696, 697, 698, 699, 700, 701, 705, 706, 711, 712, 713, 714, 727, 729, 730, 734, 735, 736, 739, 740, 742, 743, 744, 745, 746, 750, 751, 758, 759, 760, 761, 764, 765, 766, 767, 768, 769, 771, 772, 773, 780, 781, 782, 783, 784, 785, 786, 787, 788, 792, 793, 801, 802, 805, 811, 815, 816, 817, 822, 823, 832, 836, 837, 838, 842, 843, 844, 845, 849, 850, 857, 858, 859, 860, 861, 862, 864, 865, 866, 868, 873, 874, 875, 882, 886, 887, 888, 889, 890, 907, 908, 911, 912, 913, 914, 916, 922, 923, 924, 925, 929, 930]



          # The configs for the dynamics model propagation renderings
          # This is the rendering that shows how the dynamics model works
          save_model_input_output_dynamics_model_propagation_config:

            # If we should even render the sequences
            do_save: False

            # How many sequences to render
            number_to_save: 5

            # These are all the sequences that are forward facing
            sequences_to_save: [1, 5, 6, 7, 11, 12, 21, 26, 27, 28, 46, 47, 49, 50, 52, 53, 54, 59, 60, 62, 63, 64, 69, 74, 79, 85, 88, 90, 91, 96, 97, 106, 107, 114, 120, 121, 124, 125, 126, 127, 130, 131, 132, 133, 136, 137, 140, 149, 150, 158, 159, 163, 164, 165, 166, 167, 172, 173, 176, 178, 179, 180, 181, 182, 189, 191, 193, 194, 195, 196, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 212, 220, 222, 225, 232, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 263, 265, 266, 267, 268, 272, 273, 274, 275, 276, 278, 279, 280, 281, 283, 284, 287, 288, 289, 290, 291, 292, 293, 294, 296, 304, 311, 312, 314, 315, 320, 322, 323, 324, 325, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 343, 348, 349, 350, 351, 353, 354, 357, 363, 364, 365, 370, 373, 377, 378, 379, 380, 382, 383, 384, 385, 386, 388, 389, 401, 409, 410, 416, 417, 419, 420, 421, 427, 428, 430, 444, 445, 456, 463, 464, 469, 470, 471, 472, 475, 479, 483, 484, 488, 492, 493, 498, 499, 500, 501, 502, 507, 508, 509, 510, 512, 518, 522, 525, 526, 528, 534, 538, 540, 541, 542, 547, 550, 553, 554, 558, 564, 565, 572, 575, 580, 587, 594, 595, 596, 597, 599, 600, 601, 602, 609, 613, 614, 618, 619, 625, 626, 634, 635, 640, 641, 646, 647, 648, 649, 651, 655, 656, 657, 661, 662, 663, 664, 665, 666, 667, 668, 669, 674, 680, 681, 682, 685, 686, 691, 692, 695, 696, 697, 698, 699, 700, 701, 705, 706, 711, 712, 713, 714, 727, 729, 730, 734, 735, 736, 739, 740, 742, 743, 744, 745, 746, 750, 751, 758, 759, 760, 761, 764, 765, 766, 767, 768, 769, 771, 772, 773, 780, 781, 782, 783, 784, 785, 786, 787, 788, 792, 793, 801, 802, 805, 811, 815, 816, 817, 822, 823, 832, 836, 837, 838, 842, 843, 844, 845, 849, 850, 857, 858, 859, 860, 861, 862, 864, 865, 866, 868, 873, 874, 875, 882, 886, 887, 888, 889, 890, 907, 908, 911, 912, 913, 914, 916, 922, 923, 924, 925, 929, 930]

            # The name of the model to use as the dynamics model
            dynamics_model_name: "forward_dynamics_model"

            # The number of samples to draw from the dynamics model to generate the 
            # new position cloud
            number_of_samples: 10000

            # The number of initial positions to draw samples from
            number_of_initial_positions: 250

            # The KDE configs which we will use to generate the initial positions
            kde_samples_config:
               distribution_types: ["Normal", "Normal", "VonMises"]
               # bandwidths: [20.0, 20.0, 0.05]
               bandwidths: [50.0, 50.0, 0.05]







          # The configs for the sequence renderings
          sequence_rendering_config:

            # If we should even render the sequences
            do_render: False

            # How many sequences to render
            number_to_render: 10

            # The Configs for rendering the XY KDE for the sequence rendering
            xy_kde_posterior_rendering_configs:

              # Of we should render the KDE
              do_render: True

              # What dimes to extract for rendering the KDE
              # and the distribution to use when rendering for each dim
              particle_extract_dim_params:
                0: 
                  dim_in_kde: 0
                  kde_distribution_type: "Normal"
                1: 
                  dim_in_kde: 1
                  kde_distribution_type: "Normal"

            # The configs to use while rendering the angle dims KDE
            angle_kde_posterior_rendering_configs:
              
              # If we should even render the KDE
              do_render: True

              # What dimes to extract for rendering the KDE
              # and the distribution to use when rendering for each dim
              particle_extract_dim_params:
                2: 
                  dim_in_kde: 0
                  kde_distribution_type: "VonMises"

          # The configs for the dynamics model propagation renderings
          # This is the rendering that shows how the dynamics model works
          dynamics_model_propagation_rendering_config:

            # If we should even render the sequences
            do_render: False

            # How many sequences to render
            number_to_render: 5

            # The name of the model to use as the dynamics model
            dynamics_model_name: "dynamics_model"

            # Get the direction to propagate.
            # This can either be forward or backward
            direction: "forward"

            # The number of samples to draw from the dynamics model to generate the 
            # new position cloud
            number_of_samples: 25

            # The figure renderings number of rows and columns
            rows: 4
            cols: 5







        quantitative_config:
          do_run: True

          # The batch sizes per GPU
          # This will automatically scale the batch size when we scale the number of GPUs
          batch_sizes_per_gpu:
            evaluation: 2
            # evaluation: 1


          metric_configs:

            mdps_nll:
              type: kde_nll
              output_keys:
                particles: "particles"
                particle_weights: "particle_weights"
                bandwidths: "bandwidths"
              particle_extract_dim_params:
                0: 
                  dim_in_kde: 0
                  kde_distribution_type: "Normal"
                1: 
                  dim_in_kde: 1
                  kde_distribution_type: "Normal"
                2: 
                  dim_in_kde: 2
                  kde_distribution_type: "VonMises"

            mdpf_forward_nll:
              type: kde_nll
              output_keys:
                particles: "forward_particles"
                particle_weights: "forward_particle_weights"
                bandwidths: "forward_bandwidths"
              particle_extract_dim_params:
                0: 
                  dim_in_kde: 0
                  kde_distribution_type: "Normal"
                1: 
                  dim_in_kde: 1
                  kde_distribution_type: "Normal"
                2: 
                  dim_in_kde: 2
                  kde_distribution_type: "VonMises"

            mdpf_backward_nll:
              type: kde_nll
              output_keys:
                particles: "backward_particles"
                particle_weights: "backward_particle_weights"
                bandwidths: "backward_bandwidths"

              particle_extract_dim_params:
                0: 
                  dim_in_kde: 0
                  kde_distribution_type: "Normal"
                1: 
                  dim_in_kde: 1
                  kde_distribution_type: "Normal"
                2: 
                  dim_in_kde: 2
                  kde_distribution_type: "VonMises"


            position_recall:
              type: multiple_distance_recall

              # Measure the euclidean distance 
              distance_measurement_type: "Euclidean"

              # The list of metrics that we will be computing
              metrics:




                # MDPS Position
                mdps_position_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "single_point_prediction"

                # MDPF Forward Position
                forward_position_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "forward_single_point_prediction"

                # MDPF Backward Position
                backward_position_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "backward_single_point_prediction"


                # MDPS Top Modes Position
                mdps_top_modes_position_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "top_modes"


                # MDPS Top Modes Position
                forward_top_modes_position_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "forward_top_modes"



                # MDPS Top Modes Position
                backward_top_modes_position_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "backward_top_modes"




                # # MDPS Top Modes Position Refined
                # mdps_top_modes_refined_position_recall:
                #   multiple_hypothesis_selection_mode: "min"
                #   output_keys:
                #     single_point_prediction: "top_modes_refined"


                # # MDPS Top Modes Position Refined
                # forward_top_modes_refined_position_recall:
                #   multiple_hypothesis_selection_mode: "min"
                #   output_keys:
                #     single_point_prediction: "forward_top_modes_refined"



                # # MDPS Top Modes Position Refined
                # backward_top_modes_refined_position_recall:
                #   multiple_hypothesis_selection_mode: "min"
                #   output_keys:
                #     single_point_prediction: "backward_top_modes_refined"



              # Which dims of the single point prediction we want to use
              # For position we just need the first 2 dims
              single_point_prediction_dims_to_use: [0, 1]

              # Which dims of the true state to use
              # For position we just need the first 2 dims
              true_state_dims_to_use: [0, 1]

              # The thresholds to use (in meters)
              thresholds: [0, 0.1, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ,11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]





            mdps_angle_recall:
              type: multiple_distance_recall

              # Measure the euclidean distance 
              distance_measurement_type: "Angle"

              # The list of metrics that we will be computing
              metrics:

                # MDPS angle
                mdps_angle_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "single_point_prediction"

                # MDPF Forward angle
                forward_angle_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "forward_single_point_prediction"

                # MDPF Backward angle
                backward_angle_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "backward_single_point_prediction"


                # MDPS Top Modes angle
                mdps_top_modes_angle_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "top_modes"


                # MDPS Top Modes angle
                forward_top_modes_angle_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "forward_top_modes"

                # MDPS Top Modes angle
                backward_top_modes_angle_recall:
                  multiple_hypothesis_selection_mode: "min"
                  output_keys:
                    single_point_prediction: "backward_top_modes"



                # # MDPS Top Modes Angle Refined
                # mdps_top_modes_refined_angle_recall:
                #   multiple_hypothesis_selection_mode: "min"
                #   output_keys:
                #     single_point_prediction: "top_modes_refined"


                # # MDPS Top Modes Angle Refined
                # forward_top_modes_refined_angle_recall:
                #   multiple_hypothesis_selection_mode: "min"
                #   output_keys:
                #     single_point_prediction: "forward_top_modes_refined"



                # # MDPS Top Modes Angle Refined
                # backward_top_modes_refined_angle_recall:
                #   multiple_hypothesis_selection_mode: "min"
                #   output_keys:
                #     single_point_prediction: "backward_top_modes_refined"


              # Which dims of the single point prediction we want to use
              # For angle we just want the last dim
              single_point_prediction_dims_to_use: [2]

              # Which dims of the true state to use
              true_state_dims_to_use: [2]

              # The thresholds to use (in radians)
              thresholds: [0, 0.01745329, 0.05235988, 0.08726646, 0.12217305, 0.15707963, 0.19198622, 0.2268928, 0.26179939, 0.29670597, 0.33161256, 0.36651914, 0.40142573, 0.43633231, 0.4712389, 0.50614548, 0.54105207, 0.57595865, 0.61086524, 0.64577182, 0.68067841, 0.71558499, 0.75049158, 0.78539816, 0.82030475, 0.85521133, 0.89011792, 0.9250245, 0.95993109, 0.99483767, 1.02974426, 1.06465084, 1.09955743, 1.13446401, 1.1693706, 1.20427718, 1.23918377, 1.27409035, 1.30899694, 1.34390352, 1.37881011, 1.41371669, 1.44862328, 1.48352986, 1.51843645, 1.55334303]

              # The scale factor to use when displaying if needed
              # this is an optional config. Default value will be 1 if this config is not included
              thresholds_display_scale_factor: 57.2957795131



