experiment_templates:

  - Training_Template:

      # The number of times to run this experiment
      number_of_runs: <number_of_runs>

      # Specifies that we want to train and what kind of trainer to use
      application: training
      experiment_type: training
      training_type: full_sequence_trainer

      # The parameters for the device to use
      # This specifies which GPU or how many GPUs we want and also what the requirements are for the GPU
      device_configs:
        device: "cuda_auto_multi:1"
        min_free_memory_gb: 1.0
        max_number_of_tasks: 10

      # The config file for the dataset we want to use 
      dataset_configs_file: "../experiments/bearings_only/shared_configs/dataset_bearings_only_training.yaml"

      # The configs for the main model
      # Usually this just points to a model architecture config
      model_configs:

        # The name of the model architecture config to use 
        main_model_name: "main_model"

      # Configs to load models from
      # This specifies which models we should load and what save files to load for those models
      # pretrained_models:
        # observation_encoder_model: <pretrained_root_save_dir>/training/run_0000/models/best/observation_encoder_model.pt
        # weights_model: <pretrained_root_save_dir>/training/run_0000/models/best/weights_model.pt


      # Specify the configs needed for training
      training_configs:       

        # The usages configs for how we are going to use the dataset
        dataset_usage_configs:
          training_dataset_name: training
          validation_dataset_name: validation

        # The number of CPUs to use for the data loading
        # Should be less than the number of available cpus otherwise pytorch complains
        num_cpu_cores_for_dataloader: 8

        # How many epochs to train for
        epochs: 200

        # Turn off truncation 
        # truncated_bptt_modulo: -1

        # The batch sizes for training
        batch_sizes:
          training: 32
          validation: 32

        # The configs that specify early stopping
        early_stopping_configs:
          patience: 15
          start_offset: 25
          min_delta: 0
          max_lr_change: 2

        optimizer_configs:
          type: "Adam"
          weight_decay: 0.0
          gradient_clip_value: 50.0

        lr_scheduler_configs:
          type: "ReduceLROnPlateau"
          threshold: 0.001
          # threshold: 0.005
          factor: 0.1
          patience: 10
          cooldown: 4
          min_lr: 0.0
          verbose: True
          start_epoch: 0






  - Evaluation_Template:

      # The number of times to run this experiment
      number_of_runs: <number_of_runs>

      application: evaluation
      experiment_type: evaluation
      evaluation_type: bearings_only_evaluator


      # The parameters for the device to use
      # This specifies which GPU or how many GPUs we want and also what the requirements are for the GPU
      device_configs:
        device: "cuda_auto_multi:1"
        min_free_memory_gb: 1.0
        max_number_of_tasks: 10

      dataset_configs_file: "../experiments/bearings_only/shared_configs/dataset_bearings_only_evaluation.yaml"

      model_configs:
        main_model_name: "main_model"


      evaluation_configs:         

        num_cpu_cores_for_dataloader: 8

        qualitative_config:
          do_run: True

          number_to_render: 5

          kde_posterior_rendering_configs:
            do_render: True
            
            xy_kde_posterior_rendering_configs:
              particle_extract_dim_params:
                0: 
                  dim_in_kde: 0
                  kde_distribution_type: "Normal"
                1: 
                  dim_in_kde: 1
                  kde_distribution_type: "Normal"


            angle_kde_posterior_rendering_configs:
              particle_extract_dim_params:
                2: 
                  dim_in_kde: 0
                  kde_distribution_type: "VonMises"



        quantitative_config:

          do_run: True 
          batch_sizes: 
            evaluation: 16

